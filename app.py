import streamlit as st
import google.generativeai as genai
from PIL import Image
import speech_recognition as sr
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder
import tempfile
import os
import re

# --- Configuration ---
# âš ï¸ âš ï¸ âš ï¸ á€¤á€”á€±á€›á€¬á€á€½á€„á€º á€á€„á€ºá API Key á€¡á€™á€¾á€”á€ºá€€á€­á€¯ á€™á€–á€¼á€…á€ºá€™á€”á€± á€‘á€Šá€·á€ºá€•á€« âš ï¸ âš ï¸ âš ï¸
GOOGLE_API_KEY = "AIzaSyAZPKm775hHrXDatQmrLwESFVx1Xb5kiWg"

try:
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
except Exception as e:
    st.error(f"API Key Error: {e}")
    st.stop()

# --- Page Setup ---
st.set_page_config(page_title="Smart Agri Pro Voice", page_icon="ğŸŒ¾", layout="wide")

# --- Custom Title (HTML/CSS) ---
# á€á€±á€«á€„á€ºá€¸á€…á€‰á€ºá€€á€­á€¯ á€á€±á€á€•á€ºá€¡á€±á€¬á€„á€º á€•á€¼á€„á€ºá€†á€„á€ºá€á€¼á€„á€ºá€¸
st.markdown("""
    <h1 style='text-align: center; color: #2E8B57; font-size: 2.5em; font-weight: bold;'>
        ğŸŒ¾ Smart Agri - á€…á€½á€šá€ºá€…á€¯á€¶á€á€¯á€¶á€¸ á€…á€­á€¯á€€á€ºá€•á€»á€­á€¯á€¸á€›á€±á€¸á€œá€€á€ºá€‘á€±á€¬á€€á€º
    </h1>
""", unsafe_allow_html=True)

# --- Session State Management ---
if "history" not in st.session_state:
    st.session_state.history = []
if "generated_audio" not in st.session_state:
    st.session_state.generated_audio = None

# --- Helper Functions ---

def clean_text_for_speech(text):
    """AI á€™á€¾ á€•á€±á€¸á€á€±á€¬ á€…á€¬á€á€¬á€¸á€™á€»á€¬á€¸á€€á€­á€¯ á€¡á€á€¶á€™á€‘á€½á€€á€ºá€™á€® á€á€”á€·á€ºá€…á€„á€ºá€á€¼á€„á€ºá€¸"""
    clean = re.sub(r'[\*\#\-\_]', '', text)
    clean = " ".join(clean.split())
    return clean

def text_to_speech(text):
    """á€™á€¼á€”á€ºá€™á€¬á€…á€¬á€á€¬á€¸á€€á€­á€¯ á€¡á€á€¶á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    try:
        clean_text = clean_text_for_speech(text)
        tts = gTTS(text=clean_text, lang='my')
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
            tts.save(fp.name)
            return fp.name
    except:
        return None

def transcribe_audio(audio_bytes):
    """á€¡á€á€¶á€–á€­á€¯á€„á€ºá€€á€­á€¯ á€…á€¬á€á€¬á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€á€¼á€„á€ºá€¸"""
    r = sr.Recognizer()
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as fp:
        fp.write(audio_bytes)
        fp.name
    with sr.AudioFile(fp.name) as source:
        try:
            audio_data = r.record(source)
            text = r.recognize_google(audio_data, language="my-MM")
            return text
        except:
            return None
        finally:
            if os.path.exists(fp.name):
                os.remove(fp.name)

def get_ai_response(prompt, image=None):
    try:
        chat = model.start_chat(history=[])
        if image:
            response = chat.send_message([prompt, image])
        else:
            response = chat.send_message(prompt)
        return response.text
    except Exception as e:
        return f"á€…á€”á€…á€ºá€á€»á€­á€¯á€·á€šá€½á€„á€ºá€¸á€á€»á€€á€º á€›á€¾á€­á€”á€±á€•á€«á€á€Šá€º: {e}"

# --- Sidebar Controls ---
with st.sidebar:
    st.header("á€†á€€á€ºá€á€„á€ºá€™á€»á€¬á€¸ (Settings)")

    # Mode Selection
    app_mode = st.radio("á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€»á€€á€º á€›á€½á€±á€¸á€á€»á€šá€ºá€•á€«:",
        ["ğŸ¡ á€¡á€­á€™á€ºá€á€¼á€¶á€á€®á€¸á€”á€¾á€¶ (Garden)", "ğŸŒ¾ á€…á€•á€«á€¸á€…á€­á€¯á€€á€ºá€á€„á€ºá€¸ (Paddy)", "ğŸ‚ á€›á€±á€¬á€‚á€«á€…á€…á€ºá€†á€±á€¸ (Doctor)"])

    st.divider()

    # Clear Chat Button (á€¡á€›á€±á€¬á€„á€ºá€•á€¼á€±á€¬á€„á€ºá€¸á€‘á€¬á€¸á€á€Šá€º)
    if st.button("ğŸ”„ á€”á€±á€¬á€€á€ºá€á€…á€ºá€™á€»á€­á€¯á€¸ á€•á€¼á€±á€¬á€„á€ºá€¸á€™á€±á€¸á€™á€Šá€º (Clear)"):
        st.session_state.history = []
        st.session_state.generated_audio = None
        st.rerun()

    st.divider()
    enable_voice = st.checkbox("á€¡á€á€¶á€–á€¼á€„á€·á€º á€•á€¼á€”á€ºá€–á€á€ºá€•á€¼á€•á€«", value=True)
    st.info("ğŸ’¡ á€¡á€€á€¼á€¶á€•á€¼á€¯á€á€»á€€á€º: á€™á€­á€¯á€€á€ºá€á€œá€¯á€á€ºá€”á€¾á€­á€•á€ºá€•á€¼á€®á€¸ á€™á€¼á€”á€ºá€™á€¬á€œá€­á€¯ á€•á€¼á€±á€¬á€€á€¼á€¬á€¸á á€™á€±á€¸á€™á€¼á€”á€ºá€¸á€”á€­á€¯á€„á€ºá€•á€«á€á€Šá€ºá‹")

# --- Main Layout ---

# 1. Context Setting based on Mode
context_prompt = ""
user_image = None

with st.expander("ğŸ“ á€¡á€á€¼á€±á€á€¶ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€–á€¼á€Šá€·á€ºá€á€½á€„á€ºá€¸á€›á€”á€º (á€¤á€”á€±á€›á€¬á€€á€­á€¯ á€”á€¾á€­á€•á€ºá€•á€«)", expanded=True):
    col_input1, col_input2 = st.columns([2, 1])

    if app_mode == "ğŸ¡ á€¡á€­á€™á€ºá€á€¼á€¶á€á€®á€¸á€”á€¾á€¶ (Garden)":
        with col_input1:
            plant_name = st.text_input("á€¡á€•á€„á€ºá€¡á€™á€Šá€º (á€¥á€•á€™á€¬- á€›á€¯á€¶á€¸á€•á€á€®):")
            field_desc = st.text_input("á€…á€­á€¯á€€á€ºá€á€„á€ºá€¸ á€¡á€”á€±á€¡á€‘á€¬á€¸:")
        with col_input2:
            tank_size = st.number_input("á€›á€±á€€á€”á€º (á€‚á€«á€œá€¶):", value=50)

        if plant_name:
            context_prompt = f"á€¡á€•á€„á€º: {plant_name}. á€›á€±á€€á€”á€º: {tank_size} á€‚á€«á€œá€¶. á€™á€¼á€±á€¡á€”á€±á€¡á€‘á€¬á€¸: {field_desc}. (á€™á€¼á€±á€á€¼á€‡á€¬á€…á€•á€ºá€”á€Šá€ºá€¸á€”á€¾á€„á€·á€º á€•á€¼á€¯á€…á€¯á€”á€Šá€ºá€¸ á€á€½á€€á€ºá€•á€±á€¸á€•á€«)"

    elif app_mode == "ğŸŒ¾ á€…á€•á€«á€¸á€…á€­á€¯á€€á€ºá€á€„á€ºá€¸ (Paddy)":
        with col_input1:
            days = st.slider("á€…á€•á€«á€¸á€á€€á€ºá€á€™á€ºá€¸ (á€›á€€á€º):", 1, 120, 30)
            status = st.text_input("á€¡á€•á€„á€º á€¡á€á€¼á€±á€¡á€”á€±:")
        with col_input2:
            acres = st.number_input("á€…á€­á€¯á€€á€ºá€§á€€:", value=5)

        context_prompt = f"á€…á€•á€«á€¸á€á€€á€ºá€á€™á€ºá€¸: {days} á€›á€€á€º. á€…á€­á€¯á€€á€ºá€§á€€: {acres} á€§á€€. á€¡á€á€¼á€±á€¡á€”á€±: {status}. (á€œá€­á€¯á€¡á€•á€ºá€á€±á€¬ á€›á€±áŠ á€™á€¼á€±á€á€¼á€‡á€¬á€”á€¾á€„á€·á€º á€†á€±á€¸ á€¡á€€á€¼á€¶á€•á€±á€¸á€•á€«)"

    elif app_mode == "ğŸ‚ á€›á€±á€¬á€‚á€«á€…á€…á€ºá€†á€±á€¸ (Doctor)":
        uploaded_file = st.file_uploader("á€›á€±á€¬á€‚á€«á€–á€¼á€…á€ºá€”á€±á€á€±á€¬ á€•á€¯á€¶á€á€„á€ºá€•á€«:", type=["jpg", "png", "jpeg"])
        if uploaded_file:
            user_image = Image.open(uploaded_file)
            st.image(user_image, caption="á€á€„á€ºá€‘á€¬á€¸á€á€±á€¬á€•á€¯á€¶", width=200)
            context_prompt = "á€’á€®á€•á€¯á€¶á€‘á€²á€€ á€¡á€•á€„á€ºá€›á€±á€¬á€‚á€«á€€á€­á€¯ á€…á€…á€ºá€†á€±á€¸á€•á€¼á€®á€¸ á€€á€¯á€á€”á€Šá€ºá€¸ á€•á€¼á€±á€¬á€•á€¼á€•á€«á‹"

# 2. Voice Input Section (Top Area)
col_voice, col_display = st.columns([1, 4])

with col_voice:
    st.write("ğŸ™ï¸ **á€¡á€á€¶á€–á€¼á€„á€·á€º á€™á€±á€¸á€›á€”á€º:**")
    audio_blob = mic_recorder(start_prompt="ğŸ”´ Start", stop_prompt="â¬› Stop", key='recorder')

# Voice Processing Logic
voice_text = ""
if audio_blob:
    with st.spinner("á€¡á€á€¶á€–á€á€ºá€”á€±á€á€Šá€º..."):
        voice_text = transcribe_audio(audio_blob['bytes'])

# 3. Chat Interface
chat_container = st.container()

# Display History
with chat_container:
    for msg in st.session_state.history:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            if "audio_path" in msg and msg["audio_path"]:
                st.audio(msg["audio_path"], format="audio/mp3")

# 4. Handling Inputs (Voice or Text)
user_query = None

if voice_text:
    user_query = voice_text

if prompt := st.chat_input("á€á€­á€œá€­á€¯á€á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€†á€€á€ºá€œá€€á€ºá€™á€±á€¸á€™á€¼á€”á€ºá€¸á€•á€«..."):
    user_query = prompt

if user_query:
    final_prompt = user_query

    if len(st.session_state.history) == 0 and context_prompt:
        final_prompt = f"{context_prompt} \n\n á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á€™á€±á€¸á€á€½á€”á€ºá€¸: {user_query}"

    st.session_state.history.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("AI á€…á€‰á€ºá€¸á€…á€¬á€¸á€”á€±á€•á€«á€á€Šá€º..."):
            full_prompt = f"{final_prompt} (Please answer in Burmese language only. Do not include asterisks or markdown symbols in speech friendly parts.)"

            response_text = get_ai_response(full_prompt, user_image)
            st.write(response_text)

            audio_file = None
            if enable_voice:
                audio_file = text_to_speech(response_text)
                if audio_file:
                    st.audio(audio_file, format="audio/mp3")

            st.session_state.history.append({
                "role": "assistant",
                "content": response_text,
                "audio_path": audio_file
            })
